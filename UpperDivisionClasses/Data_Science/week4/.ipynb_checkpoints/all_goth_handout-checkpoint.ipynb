{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a3cb0ee3-7bca-4b2b-8a27-be198d18818e",
    "_uuid": "075ab0f3fc310e293828b3681f1d80642f88c106"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Module 4 - Gothic author identification\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "This week we are going to take on the task of identifying authors of gothic novels. Our authors to choose from are these three:\n",
    "<ol>\n",
    "<li>EAP - Edgar Allen Poe (https://en.wikipedia.org/wiki/Edgar_Allan_Poe): American writer who wrote poetry and short stories that revolved around tales of mystery and the grisly and the grim. Arguably his most famous work is the poem - \"The Raven\" and he is also widely considered the pioneer of the genre of the detective fiction.</li>\n",
    "<p>\n",
    "<li>HPL - HP Lovecraft (https://en.wikipedia.org/wiki/H._P._Lovecraft): Best known for authoring works of horror fiction, the stories that he is most celebrated for revolve around the fictional mythology of the infamous creature \"Cthulhu\" - a hybrid chimera mix of Octopus head and humanoid body with wings on the back.</li>\n",
    "<p>\n",
    "<li>MWS - Mary Shelley (https://en.wikipedia.org/wiki/Mary_Shelley): Seemed to have been involved in a whole panoply of literary pursuits - novelist, dramatist, travel-writer, biographer. She is most celebrated for the classic tale of Frankenstein where the scientist Frankenstein a.k.a \"The Modern Prometheus\" creates the Monster that comes to be associated with his name.</li>\n",
    "</ol>\n",
    "<p>\n",
    "What we have is a table of sentences from their books. Each sentence is labeled with the author who wrote it. Given a new sentence our job is to predict who the author is of that sentence.\n",
    "<p>\n",
    "The sentences are all jumbled up, i.e., we do not have paragraph or chapter level info.\n",
    "<p>\n",
    "<h2>Why is this interesting?</h2>\n",
    "<p>\n",
    "One application of this style of analysis is in literary studies. An ancient book is found but the author is unknown. Or perhaps the author is known but there is a suspicion that someone else ghost wrote it. Or even looking at plagiarism: some portions of a book by author X look like they were lifted from author Y.\n",
    "<p>\n",
    "Let's bring in the table and look at it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gothic_table = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQqRwyE0ceZREKqhuaOw8uQguTG6Alr5kocggvAnczrWaimXE8ncR--GC0o_PyVDlb-R6Z60v-XaWm9/pub?output=csv',\n",
    "                          encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gothic_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gothic_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Let's devise a plan\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "This looks similar to our tweet problem in prior weeks. We are given some text. The text has a label. Our goal is to build a model that will predict the label using the content of the text.\n",
    "<p>\n",
    "<ul>\n",
    "<li>Instead of using a bag of hashtags, let's use a bag of words.\n",
    "<p>\n",
    "<li>Naive Bayes worked well for us in tweet problem so let's try it again here.\n",
    "<p>\n",
    "<li>I want to do a bit of wrangling of the text in a sentence, more than we did for the tweets.\n",
    "</ul>\n",
    "<p>\n",
    "I'll tackle the wrangling first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Wrangling a sentence into words\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Once we start dealing with English, the complexity goes up a notch. One problem is that we will find words with apostrophes. For example, contractions like \"I'll go\", \"it's easy\", \"won't quit\". Or possesives like \"John's game\", \"Tess' party\".\n",
    "<p>\n",
    "A second problem is that we typically want to remove words that are so common they are useless in differentiating.\n",
    "Let's start with this second problem first. We will start to use the nltk package this week. nltk is like pandas in that it has lots of functions for doing a wide array of NLP tasks. For now, I know that nltk has a built-in set of words that are very common. They are called \"stop words\" (https://en.wikipedia.org/wiki/Stop_words). The general idea is that we want to delete these words from a sentence before doing any analysis.\n",
    "<p>\n",
    "Here they are.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  # see more at http://xpo6.com/list-of-english-stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u\"you'll\",\n",
       " u\"you'd\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swords = stopwords.words('english')\n",
    "swords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "If you scroll through them, you will see the contractions. But with a big caveat: You will see pieces of the contraction but not the full contraction. For instance, you see \"ll\" I assume from \"I'll\". You see \"doesn\" I assume from \"doesn't\". What does this mean? It means that some other wrangling tool must be applied before we start looking for stop words. That tool is called a word tokenizer. nltk also has a sentence tokenizer but we don't need that - some nice person already broke the books into sentences for us. The word tokenizer takes a sentence as input and produces a list of words. nltk has several word tokenizers built in. Let's look at 2 of them below.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()          #instantiate class\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treeb_tokenizer = TreebankWordTokenizer()            #instantiate class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "I am going to have a bake-off between the 2 tokenizers. What I want is to use a tokenizer and then remove the stop words from the tokenized list: a tokenizer produces a list of words. I'll try a test sentence out with each tokenizer and get its list of words. I'll then run through the stop words to see how many I remove.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'\n",
      "ll\n",
      "say\n",
      "it\n",
      "'\n",
      "s\n",
      "6\n",
      "o\n",
      "'\n",
      "clock\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#First up: the punctuation tokenizer\n",
    "\n",
    "test_sentence = \"I'll say it's 6 o'clock!\"\n",
    "\n",
    "word_tokes = word_punct_tokenizer.tokenize(test_sentence)\n",
    "for item in word_tokes:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "You can see it treats an apostrophe as a separate \"word\". So \"I'll\" becomes three words: \"I\", \"'\", \"ll\". This feels like what we want to match against stop words. Let's do that now and see how many we match.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "s\n",
      "ll\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "#How many matches in stop words?\n",
    "\n",
    "for word in swords:\n",
    "    c = word_tokes.count(word)\n",
    "    if c > 0:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "I like the first 3. Would have preferred \"oclock\" instead of \"o\", \"'\", \"clock\". And we still have the apostrophes in the list - 3 of them. We can deal with them later.\n",
    "<p>\n",
    "Next batter up.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'ll\n",
      "say\n",
      "it\n",
      "'s\n",
      "6\n",
      "o'clock\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#http://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "word_tokes = treeb_tokenizer.tokenize(test_sentence)\n",
    "for item in word_tokes:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Not looking good. We will not match \"'ll\" nor \"'s\" I predict. However, I do like that o'clock stays together.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n"
     ]
    }
   ],
   "source": [
    "for word in swords:\n",
    "    c = word_tokes.count(word)\n",
    "    if c > 0:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Only one word removed. The winner, at least in terms of stop word matching, is the punct tokenizer. So I'll use that.\n",
    "<p>\n",
    "As an aside, there is nothing magical about tokenizers. You can see their code with a little digging. They mostly are made up of a bunch of re pattern matches. Nothing stopping you from extending a tokenizer to your own taste. For instance, would not be hard to change one-word contractions into their full two-word equivalent using the re sub method.\n",
    "<p>\n",
    "Aside part 2: you can check out how various nltk tokenizers do on sentences you type in here: \n",
    "http://textanalysisonline.com/nltk-word-tokenize\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 1\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I'd like you to work on a function `sentence_wrangler`. It will take a raw sentence from a row and tokenize it. It will then remove the following from that word list:\n",
    "<p>\n",
    "<ul>\n",
    "<li>The stop words we have been using.\n",
    "<p>\n",
    "<li>Words that contain any punctuation (see string package).\n",
    "<p>\n",
    "</ul>\n",
    "<p>\n",
    "Have it return 2 lists for debugging: the list of wrangled words and the list of removed words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    word_tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    removed = []\n",
    "    wrangled = []\n",
    "    for word in word_tokes:\n",
    "        word = word.lower()\n",
    "        if word in swords or all(char in punctuation for char in word):\n",
    "            removed.append(word)\n",
    "        else:\n",
    "            wrangled.append(word)\n",
    "    return(wrangled, removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['say', '6', 'clock'], ['i', \"'\", 'll', 'it', \"'\", 's', 'o', \"'\", '!'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_wrangler(test_sentence, swords, punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Ok, let's try it on first 10 sentences in the table. I'll print out the raw sentence and then the words I remove.\n",
    "<p>\n",
    "If you are matching my results, move on to challenge 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n",
      "this , , me no of the of my ; as i its , and to the i out , being of the ; so the .\n",
      "==========\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "it once to me that the be a .\n",
      "==========\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n",
      "\n",
      "in his was a , from which , as he down the , all of , he with an of the .\n",
      "==========\n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n",
      "how is as we from on the , by and , all as in , and .\n",
      "==========\n",
      "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.\n",
      "\n",
      ", not , the his ; but a over his as he at his .\n",
      "==========\n",
      "A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.\n",
      "\n",
      "a in , my under your and , has so the of my that i an to the on : i have it to be , and when i of a for his of and the and to him by his , i myself in being to his .\n",
      "==========\n",
      "The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.\n",
      "\n",
      "the , , at this , in the of ; and here was .\n",
      "==========\n",
      "The surcingle hung in ribands from my body.\n",
      "\n",
      "the in from my .\n",
      "==========\n",
      "I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.\n",
      "\n",
      "i that you not to yourself ' ' being to of , and of the of ; and , when we this not very , i to you how , with how , the of that had with in the , i that you not your to the in , and i that you do so .\n",
      "==========\n",
      "I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.\n",
      "\n",
      "i that the of , nor the of , nor the of for me .\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    text = gothic_table.loc[i, 'text']\n",
    "    print(text+'\\n')\n",
    "    print(' '.join(sentence_wrangler(text, swords, punctuation)[1]).encode('ascii'))\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>\n",
    "Challenge 2\n",
    "\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "Fill out `all_words` below to produce the bag of words. Use your sentence_wrangler.\n",
    "<p>\n",
    "Remember that we now have 3 predicted values. So you will need to follow each word with a list of 3 numbers. Make the first number in list a count of EAP, the second number a count of HPL and the third number the count of MWS.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words(table, swords, punctuation):\n",
    "    all_author_words = {}\n",
    "    for i, row in table.iterrows():\n",
    "        wrangled_sentence = sentence_wrangler(row['text'], swords, punctuation)[0]\n",
    "        for word in wrangled_sentence:\n",
    "            if word not in all_author_words.keys():\n",
    "                all_author_words[word] = [0, 0, 0]        \n",
    "            if row['author'] == 'EAP':\n",
    "                all_author_words[word][0]+=1\n",
    "            elif row['author'] == 'HPL':\n",
    "                all_author_words[word][1]+=1\n",
    "            else:\n",
    "                all_author_words[word][2]+=1\n",
    "    return all_author_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24944"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = all_words(gothic_table, swords, punctuation)\n",
    "len(bag_of_words)  #unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Do you match my length?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "If not, your `sentence_wrangler` is not matching mine I suspect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aaem', [1, 0, 0]),\n",
       " (u'ab', [1, 0, 0]),\n",
       " (u'aback', [2, 0, 0]),\n",
       " (u'abaft', [0, 0, 1]),\n",
       " (u'abandon', [7, 3, 1])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bag_of_words.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Do you match my content?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "If not, you might have list ordering screwed up in `all_words`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Challenge 3\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "Let's take a look at words that are odd. Build a list of keys in the bag of words that contain at least one character that is not a letter. I am calling these odd words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build odd_words\n",
    "odd_words = []\n",
    "for key in bag_of_words.iterkeys():\n",
    "    if key.encode('utf-8').isalpha() != True:\n",
    "        odd_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (odd_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'attach\\xe9s',\n",
       " u'atr\\xe9e',\n",
       " u'be\\xeblzebub',\n",
       " u'tun\\xe9d',\n",
       " u'indagin\\xe9',\n",
       " u'mu\\xf1oz',\n",
       " u'abb\\xe9',\n",
       " u'\\xe6dile',\n",
       " u'pr\\xe6texta',\n",
       " u'vari\\xe9t\\xe9s',\n",
       " u'\\xe6rial',\n",
       " u'caf\\xe9',\n",
       " u'pr\\xe9par\\xe9es',\n",
       " u'pr\\xe6ter',\n",
       " u'm\\xe9nageais',\n",
       " u'recherch\\xe9s',\n",
       " u'chauss\\xe9e',\n",
       " u'inerti\\xe6',\n",
       " u'th\\xe9\\xe2tre',\n",
       " u'dor\\xe9',\n",
       " u'\\xe9meutes',\n",
       " u'sant\\xe9',\n",
       " u'andr\\xe9e',\n",
       " u'maelstr\\xf6m',\n",
       " u'c\\xe6lius',\n",
       " u'id\\xe9e',\n",
       " u'd\\xe6monic',\n",
       " u'b\\xeatenoir',\n",
       " u'antenn\\xe6',\n",
       " u're\\xebntered',\n",
       " u'dr\\xf4mes',\n",
       " u'a\\xebrially',\n",
       " u'pari\\xe8r',\n",
       " u'i\\xe4',\n",
       " u'c\\xe9der',\n",
       " u'cimabu\\xe9',\n",
       " u'm\\xfcller',\n",
       " u'celepha\\xefs',\n",
       " u'mos\\xe4iques',\n",
       " u'\\xe6ronaut',\n",
       " u'ch\\xe2teau',\n",
       " u'voil\\xe0',\n",
       " u'pr\\xe6valent',\n",
       " u'scarab\\xe6i',\n",
       " u'\\xe6ronauts',\n",
       " u't\\xeate',\n",
       " u're\\xebmbarked',\n",
       " u'\\u03bf\\u1f36\\u03b4\\u03b1',\n",
       " u'rog\\xeat',\n",
       " u'co\\xf6rdinated',\n",
       " u'pal\\xe6',\n",
       " u'baiss\\xe9e',\n",
       " u'se\\xf1or',\n",
       " u'tych\\xe9',\n",
       " u'distingu\\xe9',\n",
       " u'mus\\xe9e',\n",
       " u'dun\\xf4t',\n",
       " u'\\xe6rostation',\n",
       " u'\\u03c5\\u03c0\\u03bd\\u03bf\\u03c3',\n",
       " u'r\\xf4le',\n",
       " u'encyclop\\xe6dias',\n",
       " u'qu\\xe6stor',\n",
       " u'encyclop\\xe6dia',\n",
       " u'tir\\xe9',\n",
       " u'pens\\xe9',\n",
       " u'laoco\\xf6n',\n",
       " u're\\xe7ue',\n",
       " u'markbr\\xfcnnen',\n",
       " u'm\\xeame',\n",
       " u'petitma\\xeetre',\n",
       " u'otaheit\\xe9',\n",
       " u'b\\xeate',\n",
       " u'\\xe0',\n",
       " u'\\xe5ngstrom',\n",
       " u'pav\\xe9e',\n",
       " u'br\\xe8ve',\n",
       " u'\\xe6milianus',\n",
       " u'prim\\xe6val',\n",
       " u'outr\\xe9',\n",
       " u'fricass\\xe9e',\n",
       " u'ann\\xe6us',\n",
       " u'velout\\xe9',\n",
       " u'r\\xe8duit',\n",
       " u'barri\\xe8re',\n",
       " u'cr\\xe9billon',\n",
       " u'recherch\\xe9',\n",
       " u'c\\xf6incident',\n",
       " u'troisi\\xeame',\n",
       " u'\\xe6schylus',\n",
       " u'fianc\\xe9',\n",
       " u'p\\xe8re',\n",
       " u'sacr\\xe9',\n",
       " u'\\xe6neid',\n",
       " u'habitu\\xe9s',\n",
       " u'gr\\xe2ve',\n",
       " u'a\\xebrial',\n",
       " u'str\\xf6m',\n",
       " u'\\xe6gyptus',\n",
       " u'scarab\\xe6us',\n",
       " u'am\\xe8rement',\n",
       " u'olatho\\xeb',\n",
       " u'nebul\\xe6',\n",
       " u'pr\\xe6ternatural',\n",
       " u'\\xe9lite',\n",
       " u'pr\\xe6torian',\n",
       " u'propr\\xe6tor',\n",
       " u'is\\xe6us',\n",
       " u'soir\\xe9e',\n",
       " u'a\\xebroplane',\n",
       " u'd\\xe9shabill\\xe9']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h1>\n",
    "<div class=h1_cell>\n",
    "These are words that slipped through our `sentence_wrangler`.\n",
    "You can look the byte codes up, e.g., google for \"\\xe9\". I suppose we could add further wrangling to `sentence_wrangler` at this point to clean up even more punctuation, but I am ready to move on.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 4\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Get ready for Naive Bayes. What are we missing? We have bag_of_words that gives us the triple values we need. We are missing `P(O)`: the total count of the sentences for each author. Build that now in `total_count`.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7900, 5635, 6044]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = [0, 0, 0]\n",
    "for i, row in gothic_table.iterrows():\n",
    "    if row['author'] == 'EAP':\n",
    "        total_count[0] += 1\n",
    "    elif row['author'] == 'HPL':\n",
    "        total_count[1] += 1\n",
    "    else:\n",
    "        total_count[2] += 1\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 5\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Ok, let's get to it. Define `naive_bayes_gothic`. Fill in my function below and match my results. As last week, I expect your function to return the 3 probabilities for each of EAP, HPL, MWS.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added in the needed arguments to call sentence_wranger\n",
    "def naive_bayes_gothic(raw_sentence, bag, counts, swords, punctuation):\n",
    "    valuesInText = sentence_wrangler(raw_sentence, swords, punctuation)[0]\n",
    "    countValues = [1] * len(counts)\n",
    "    # probValues = [0] * len(counts)\n",
    "    for i in range(len(valuesInText)):\n",
    "        if valuesInText[i] in bag.keys():\n",
    "            for j in range(len(counts)):\n",
    "                countValues[j] *= bag[valuesInText[i]][j] / float(counts[j])\n",
    "    \n",
    "    total = 0\n",
    "    for val in counts:\n",
    "        total += val\n",
    "    \n",
    "    probValues = [count / float(total) for count in counts]\n",
    "    \n",
    "    finalValues = [0] * len(counts)\n",
    "    for i in range(len(counts)):\n",
    "        finalValues[i] = countValues[i] * probValues[i]    \n",
    "    \n",
    "    return tuple(finalValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1091900736782457e-50, 0.0, 0.0)\n",
      "EAP\n",
      "(0.0, 2.757386334036038e-15, 0.0)\n",
      "HPL\n",
      "(4.709053372343932e-49, 0.0, 0.0)\n",
      "EAP\n",
      "(0.0, 0.0, 6.192200285854468e-53)\n",
      "MWS\n",
      "(0.0, 3.1095121234157133e-44, 0.0)\n",
      "HPL\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(naive_bayes_gothic(gothic_table.loc[i, 'text'], bag_of_words, total_count, swords, punctuation))\n",
    "    print(gothic_table.loc[i, 'author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=h1_cell>\n",
    "Not bad. Five out of five correct. Notice all those zeros. We are doing well because we are finding a word that only appears in the book of a specific author. For example, I can see under challenge 2 that `abaft` only appears in MWS. Hence, P(abaft|EAP) and P(abaft|HPL) will both be 0. That will zero-out the numerator (no matter how many other words do match) and return 0 as result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 6\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Generate your predictions, get actuals and zip it up. I ended up timing prediction generation but it took only 13 seconds. Gotta love NB and use of fast dictionary look-up in Python.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "312.378397942\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "predictions = []\n",
    "for i,row in gothic_table.iterrows():\n",
    "    if i%1000 == 0: print('did 1000')\n",
    "    pair = naive_bayes_gothic(gothic_table.loc[i, 'text'], bag_of_words, total_count, swords, punctuation)\n",
    "    if pair[0] >= pair[1] and pair[0] >= pair[2]:\n",
    "        predictions.append(0)\n",
    "    elif pair[1] > pair[0] and pair[1] >= pair[2]:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(2)\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)  # in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Go ahead and build `zipped`.\n",
    "<p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build zipped\n",
    "actuals = []\n",
    "for i in range(len(gothic_table)):\n",
    "    if gothic_table.loc[i, 'author'] == \"EAP\":\n",
    "        actuals.append(0)\n",
    "    elif gothic_table.loc[i, 'author'] == \"HPL\":\n",
    "        actuals.append(1) \n",
    "    else:\n",
    "        actuals.append(2)\n",
    "zipped = zip(predictions,actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-1ec70cb29c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfusion_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzipped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconfusion_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconfusion_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mconfusion_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (2, 2)"
     ]
    }
   ],
   "source": [
    "confusion_dictionary = {(0, 0): 0, (0, 1): 0, (1, 0): 0, (1, 1): 0, (0,2): 0, (2,0): 0, (2, 2): 0}\n",
    "for pair in zipped:\n",
    "    confusion_dictionary[pair] += 1\n",
    "correct = (1.0*confusion_dictionary[(0,0)]+confusion_dictionary[(1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349302824454773"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0*correct/len(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Go Naive Bayes!\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I'm claiming 94% accuracy. I like it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Multinomial versus Bernoulli\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "We are using Multinomial Naive Bayes because we are counting how many times a word occurs for an author. We could also use Bernoulli Naive Bayes where we look for features that are true or false, e.g., a sentence is greater than 10 words in length. This paper discusses the difference between the two: http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Write your bag_of_words out to file\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "Here is code I used to write my bag_of_words out to file. I then read it back in just to make sure of round-trip. You should do the same. You will need this file for the midterm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bag_of_words.txt', 'w') as file:\n",
    "    file.write(json.dumps(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag2 = json.load(open(\"bag_of_words.txt\"))  # making sure I can read it in again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aaem', [1, 0, 0]),\n",
       " (u'ab', [1, 0, 0]),\n",
       " (u'aback', [2, 0, 0]),\n",
       " (u'abaft', [0, 0, 1]),\n",
       " (u'abandon', [7, 3, 1])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bag2.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag2 == bag_of_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
