{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "    border-style: groove;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "    border-style: groove;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Final Project. Callin out Twitter Haters\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Using the bag of tweets from week 3 as our training data, I will scrape Twitter's trending hashtags/stories and find haters. From the list of haters that are found from trending, I'll post to a status here: https://twitter.com/HDetection/with_replies. You'll need to install the following packages that are listed in the code below\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install twitter\n",
    "# pip install msgpack\n",
    "# pip install argparse\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import twitter\n",
    "from twitter import *\n",
    "import json\n",
    "home_path =  os.path.expanduser('~')\n",
    "file_path = '/Documents/CIS/UpperDiv/CIS_399/final/'\n",
    "file_name = 'tweet_table.csv'\n",
    "\n",
    "tweet_table = pd.read_csv(home_path + file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"Top Secret\"\n",
    "token_secret = \"Hush Hush information\"\n",
    "consumer_key = \"Shhhhhhhh\"\n",
    "consumer_secret = \"Secret McSecretFace\"\n",
    "t = Twitter(\n",
    "    auth=OAuth(token, token_secret, consumer_key, consumer_secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>hash_count</th>\n",
       "      <th>#trump</th>\n",
       "      <th>#politics</th>\n",
       "      <th>#allahsoil</th>\n",
       "      <th>#libtard</th>\n",
       "      <th>#liberal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so...</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  length  \\\n",
       "0   1      0  @user when a father is dysfunctional and is so...     101   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...     122   \n",
       "2   3      0                                bihday your majesty      19   \n",
       "3   4      0  #model   i love u take with u all the time in ...     116   \n",
       "4   5      0             factsguide: society now    #motivation      38   \n",
       "\n",
       "   bang_count  hash_count  #trump  #politics  #allahsoil  #libtard  #liberal  \n",
       "0           0           1       0          0           0         0         0  \n",
       "1           0           3       0          0           0         0         0  \n",
       "2           0           0       0          0           0         0         0  \n",
       "3           3           1       0          0           0         0         0  \n",
       "4           0           1       0          0           0         0         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Set up from earlier in the term. Naive Bayes, count all hashes, useful counts, etc.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_hashes(table):\n",
    "    hashes_Dict = {}\n",
    "    hashTags = []\n",
    "    hash_pat = r\"#\\w+\"\n",
    "    for i in range(len(table)):\n",
    "        hashTags = re.findall(hash_pat, table.iloc[i]['tweet'].lower())\n",
    "        label = table.iloc[i]['label']\n",
    "        for j in range(len(hashTags)):\n",
    "            if hashTags[j] not in hashes_Dict:\n",
    "                hashes_Dict[hashTags[j]] = [0, 0]\n",
    "            hashes_Dict[hashTags[j]][label] += 1\n",
    "    return hashes_Dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_count': (29720, 2242),\n",
       " 'class_prob': (0.9298542018647143, 0.07014579813528565),\n",
       " 'tweet_count': 31962}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_counts = {}\n",
    "hateCount = 0\n",
    "noHateCount = 0\n",
    "for i in range(len(tweet_table)):\n",
    "    if (tweet_table.iloc[i]['label'] == 0):\n",
    "        noHateCount += 1\n",
    "    else:\n",
    "        hateCount += 1\n",
    "\n",
    "total_tweets = len(tweet_table)\n",
    "\n",
    "useful_counts['class_count'] = (noHateCount, hateCount)\n",
    "\n",
    "noHateProb = noHateCount / float(total_tweets)\n",
    "hateProb = hateCount / float(total_tweets)\n",
    "\n",
    "useful_counts['class_prob'] = (noHateProb, hateProb)\n",
    "useful_counts['tweet_count'] = total_tweets\n",
    "\n",
    "useful_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_count': (29720, 2242),\n",
       " 'class_prob': (0.9298542018647143, 0.07014579813528565),\n",
       " 'naked_count': (7933, 608),\n",
       " 'tweet_count': 31962}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashTags = []\n",
    "hash_pat = r\"#\\w+\"\n",
    "nakedNoHate = 0\n",
    "nakedHate = 0\n",
    "for i in range(len(tweet_table)):\n",
    "    hashTags = re.findall(hash_pat, tweet_table.iloc[i]['tweet'].lower())\n",
    "    label = tweet_table.iloc[i]['label']\n",
    "    if(label == 0 and len(hashTags) == 0):\n",
    "        nakedNoHate += 1\n",
    "    elif(label == 1 and len(hashTags) == 0):\n",
    "        nakedHate += 1\n",
    "\n",
    "useful_counts['naked_count'] = (nakedNoHate, nakedHate)\n",
    "useful_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Problem: everytime we load this notebook, we lost our data from our previous session, all the hashtags/useful_counts get reset </p>\n",
    "<br>\n",
    "<p>Solution: read in files that I wrote out that saves this data</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of saved hashtags, this includes those from the table and new ones found from twitter\n",
    "# saved useful_counts so that way we don't start fresh everytime we load up this tool\n",
    "all_hashes = json.load(open(\"bag_of_hashes.txt\"))\n",
    "useful_counts = json.load(open(\"useful_counts.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>\n",
    "    Bring in Naive Bayes and create a function called update_useful_counts to account for the new tweets being found\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_useful_counts(count_dict, hate_value):\n",
    "        newClassCount = [count_dict['class_count'][0], count_dict['class_count'][1]]\n",
    "        newClassCount[hate_value] += 1\n",
    "        count_dict['class_count'] = tuple(newClassCount)\n",
    "        newClassProb = [count_dict['class_count'][0] / float(count_dict['tweet_count']), count_dict['class_count'][1] / float(count_dict['tweet_count'])]\n",
    "        count_dict['class_prob'] = tuple(newClassProb)\n",
    "\n",
    "hash_pat = r\"#\\w+\"\n",
    "hash_patc = re.compile(hash_pat)\n",
    "def naive_bayes(text, count_dictionary, patc, bag_of_words, class_size):\n",
    "    valuesInText = re.findall(patc, text)\n",
    "    countValues = [1] * class_size\n",
    "    for i in range(len(valuesInText)):\n",
    "        if valuesInText[i] in bag_of_words:\n",
    "            for j in range(class_size):\n",
    "                countValues[j] *= bag_of_words[valuesInText[i]][j] / float(count_dictionary['class_count'][j])\n",
    "    \n",
    "    if (len(valuesInText) == 0):\n",
    "        for i in range(class_size):\n",
    "            countValues[i] *= count_dictionary['naked_count'][i] / float(count_dictionary['class_count'][i])\n",
    "    \n",
    "    finalValues = [0] * class_size\n",
    "    for i in range(class_size):\n",
    "        finalValues[i] = countValues[i] * count_dictionary['class_prob'][i]\n",
    "    \n",
    "    # i can add this in since i will no longer be using the training data and will be reading in real data. If a tweet has no hashtags\n",
    "    # then after computing its value as either hate or not, we should increment the naked_count\n",
    "    hateVal = 0 if finalValues[0] > finalValues[1] else 1\n",
    "    if len(valuesInText) == 0:\n",
    "        newNakedCount = [count_dictionary['naked_count'][0], count_dictionary['naked_count'][1]]\n",
    "        newNakedCount[hateVal] += 1\n",
    "        count_dictionary['naked_count'] = tuple(newNakedCount)\n",
    "    for value in valuesInText:\n",
    "        if value not in bag_of_words:\n",
    "            bag_of_words[value] = [0,0]\n",
    "        bag_of_words[value][hateVal] += 1\n",
    "    return tuple(finalValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Bourdain\n",
      "#NationalBestFriendsDay\n",
      "#WorldOceansDay\n",
      "Charles Krauthammer\n",
      "Kids See Ghosts\n",
      "#FridayFeeling\n",
      "#HalloweenMovie\n",
      "Gronk\n",
      "Muhammad Ali\n",
      "#FlashbackFriday\n",
      "Justice Sotomayor\n",
      "Michael Busch\n",
      "ON SALE NOW\n",
      "Yoel Romero\n",
      "Caden O'Brien\n",
      "Tar Heels\n",
      "Delpo\n",
      "Soviet-style\n",
      "#AnnoyATVShow\n",
      "#USCM2018\n",
      "#NewMusicFriday\n",
      "#StanleyCupChampions\n",
      "#MyStory\n",
      "#BreyersBFF\n",
      "#LMS18\n",
      "#EndTheStigma\n",
      "#GhostbustersDay\n",
      "#ImpactThatMatters\n",
      "#DXC600\n",
      "#TravelSkills\n",
      "#FurnitureASongOrBand\n",
      "#MIT2018\n",
      "#VHSL\n",
      "#OMCchat\n",
      "#JakksIncredibles2\n"
     ]
    }
   ],
   "source": [
    "#list of trends for the united states\n",
    "results = t.trends.place(_id=23424977)\n",
    "for location in results:\n",
    "    for trend in location['trends']:\n",
    "        print trend['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>\n",
    "        Loop through trends in the United States, find tweets that are in English (I think I may have labeled someone from a scandinavian country a hater despite not knowing what the tweet said, so we now search solely for english tweets), go through the tweets and put them through naive_bayes, if a hate tweet is found then we print the tweet and append to our haterList and lastly we update our useful_counts. Possible that no haters will be found from one go.\n",
    "    <br><br>EDIT: Made this into a function and have it loop until we hit the rate limit. :)\n",
    "    </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# woeid for the united states\n",
    "# very well could find no haters since the # of tweets returned only gives about ~150 total tweets across all tweets\n",
    "haterList = []\n",
    "def scrapeTwitter():\n",
    "    trend_result = t.trends.place(_id=23424977)\n",
    "    badTweetList = []\n",
    "    while True:\n",
    "        for location in trend_result:\n",
    "            for trending in location['trends']:\n",
    "                try:\n",
    "                    tweets = t.search.tweets(q=trending['name'], lang='en')\n",
    "                except:\n",
    "                    print \"rate limit reached\"\n",
    "                    return badTweetList\n",
    "                for statuses in tweets['statuses']:\n",
    "                    value = naive_bayes(statuses['text'], useful_counts, hash_pat, all_hashes, 2)\n",
    "                    useful_counts['tweet_count'] += 1\n",
    "                    if value[1] > value[0]:\n",
    "                        badTweetList.append(statuses['text'])\n",
    "                        badTweetList = list(set(badTweetList))\n",
    "                        haterList.append(statuses['user']['screen_name'])\n",
    "                    hateVal = 0 if value[0] > value[1] else 1\n",
    "                    if statuses['text'] not in tweetList:\n",
    "                        update_useful_counts(useful_counts, hateVal) \n",
    "    return badTweetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate limit reached\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "tweetList = scrapeTwitter()\n",
    "haterList = list(set(haterList))\n",
    "print tweetList\n",
    "print haterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veto(hatersToRemove):\n",
    "    for hater in hatersToRemove:\n",
    "        haterList.remove(hater)\n",
    "\n",
    "def reverseUseful():\n",
    "    reversedCount = [useful_counts['class_count'][0]+1, useful_counts['class_count'][1]-1]\n",
    "    useful_counts['class_count'] = tuple(reversedCount)\n",
    "    correctedProb = [useful_counts['class_count'][0] / float(useful_counts['tweet_count']), useful_counts['class_count'][1] / float(useful_counts['tweet_count'])]\n",
    "    useful_counts['class_prob'] = tuple(correctedProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ad1678d7ed4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnotHaters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnotHaters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotHaters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mreverseUseful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b018411ea11d>\u001b[0m in \u001b[0;36mveto\u001b[0;34m(hatersToRemove)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhatersToRemove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhater\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhatersToRemove\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mhaterList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhater\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreverseUseful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "# ONLY NEEDED IF WE DETERMINE A USER IS MIS-LABELED A HATER\n",
    "notHaters = []\n",
    "notHaters.append(None)\n",
    "veto(notHaters)\n",
    "reverseUseful()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>\n",
    "        Loop through our haters, put a status on twitter, then clear out our haterList so we can find new ones. All evidence of haters can be found in the twitter link posted at the top :)\n",
    "    </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post message to twitter saying that a hater has been found\n",
    "for hater in haterList:\n",
    "    message = \"@\" + hater + \" has been found to be a hater\"\n",
    "    try:\n",
    "        t.statuses.update(status=message)\n",
    "    except:\n",
    "        print hater + ' status already exists'\n",
    "\n",
    "# clear out the haterList for new haters to be found.\n",
    "haterList = []\n",
    "tweetList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'class_count': (43524, 2307),\n",
       " u'class_prob': (0.9496007330802461, 0.05033381332635162),\n",
       " u'naked_count': (19712, 608),\n",
       " u'tweet_count': 45834}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updated useful_counts\n",
    "useful_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "    write our new list of hashtags and our most current useful_counts out to their own files so we don't lose important data\n",
    "    </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bag_of_hashes.txt', 'w') as file:\n",
    "    file.write(json.dumps(all_hashes))\n",
    "    \n",
    "with open('useful_counts.txt', 'w') as file:\n",
    "    file.write(json.dumps(useful_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
